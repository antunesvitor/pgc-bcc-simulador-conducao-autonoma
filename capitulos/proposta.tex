\chapter{Modelo proposto}\label{cap:proposta}
Este capítulo abordará o sistema de treinamento proposto. Primeiramente será exposto como funciona todo o ambiente desenvolvido, depois será explicado sobre a configuração dos algoritmos; por fim é explicado como será o treinamento e teste dos agentes. 

% Esta seção está sujeita a mudanças feitas no ambiente durante o PGCIII. 
\section{Modelo}\label{modelo}
Aqui é descrito sobre o modelo do projeto. Por ``modelo'' entende-se o conjunto ambiente-agente-recompensas. Cada um deles é detalhado nas subseções abaixo. Dentre os módulos que compõe um veículo autônomo o foco deste projeto será em desenvolver o componente de controle de deslocamento que é responsável por fazer carro percorrer um trajeto. No mundo real as rotas seriam geradas por outro componente, porém isto está além do escopo deste projeto, portanto elas serão definidas pelo autor. Importante ressaltar que o conjunto de rotas deve ser composto por uma variedade de trajetos que exijam uma diversidade de manobras a serem aprendidas pelo agente.

\subsection{O ambiente}
A simulação envolve treinar o veículo para percorrer trajetos em ambiente urbano, a princípio, por uma questão de simplicidade, o agente não terá de lidar com declives ou aclives, semáforos, outros veículos ou pedestres. Portanto, o foco poderá se manter no estudo de fazer um agente percorrer as rotas. Às bordas do cenário há muros que limitam o alcance do veículo. Como os trajetos desta proposta são curtos, não há necessidade de um modelo muito grande, porém pode vir a ser expandido em estudos futuros que visem trajetos longos. O agente recebe uma punição ao se chocar com a calçada ou com os muros da borda da cidade.

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.45, angle=90]{figs/Mapa-simulador-visao-superior.png}
    \caption{Visão superior o cenário urbano criado para o treinamento do veículo}
    \label{fig:map-view}
 \end{figure}

As rotas são compostas por: origem, \textit{checkpoints} e destino. O primeiro indica o local de partida do veículo, o último é o objetivo final do agente naquele episódio. Os \textit{checkpoints} são barreiras que indicam o caminho que deve ser percorrido, cada vez que o veículo atravessa uma delas ele ganha uma recompensa, isto é uma forma de indicar a ele que está fazendo o correto. Há 17 percursos predefinidos, cada um deles possuem características distintas como distância origem-destino, quais e quantas conversões a serem feitas, isto foi moldado visando trazer uma diversidade maior de desafios a serem superados pelo agente. Assim como o tamanho do cenário, novos trajetos podem ser considerados em estudos futuros.

\begin{figure}[h]
   \centering
   \includegraphics[scale=0.35]{figs/detalhe-rota.png}
    \caption{Rota vista de perspectiva isométrica, o agente posicionado a origem ao canto esquerdo, com os \textit{checkpoints} ao longo do percurso até o destino no canto direito.}
    \label{fig:route-view}
 \end{figure}

\subsection{O agente}\label{agent-subsection}
O agente possui 20 sensores de distância divididos em 2 conjuntos. Estes sensores são um \textit{component} do pacote \textbf{ML-agents} da Unity3D e além da distância são capazes de distinguir qual o objeto. Estes sensores dentro do editor são representados por feixes que partem do centro do veículo, é possível configurar diversos atributos deles como a quantidade, o ângulo máximo de distância do primeiro ao último, o ângulo vertical (que indica se eles apontam para cima ou para baixo) e o tamanho da esfera, que nada mais é que a tolerância de colisão do sensor.

O primeiro conjunto fica à altura da visão do motorista e servem principalmente para detectar os \textit{checkpoints}. Este possui 9 sensores, um frontal e 8 dispostos em uma faixa de 80° (4 para cada lado a partir do frontal). O segundo conjunto fica mais próximo ao solo, seu propósito principal é detectar a calçada e as paredes. Este possui 11 sensores, um frontal e 10 dispostos em uma faixa de 170°, assim ele é possível detectar obstáculos nas laterais do veículo bem como na parte traseira. Na \hyperref[fig:casting-rays]{figura} \ref{fig:casting-rays}, é possível ver os feixes partindo do carro, em verde são os que pertencem ao primeiro conjunto se chocando em um \textit{checkpoint}. Em vermelho, está o segundo conjunto com alguns de seus raios se chocando a parede.

 \begin{figure}[h]
   \centering
   \includegraphics[width=15cm]{figs/agente-raios-checkpoint.png}
    \caption{Exemplo do agente e seus raios perceptores, é possível ver os dois conjuntos de sensores se chocando com objetos do ambiente.}
    \label{fig:casting-rays}
 \end{figure}

A quantidade de sensores e sua distribuição foram decididas após alguns testes manuais do autor, com a intenção de evitar pontos cegos porém sem um número excessivo de observações causando lentidão no treinamento. Além dos raios perceptores, foi adicionado a velocidade do veículo à lista de observações do agente. Vale lembrar que o não foi implementado um recurso de imagem em um primeiro momento, ou seja, o veículo só enxerga através dos sensores e tem ciência de sua própria velocidade, ele é ``cego'' a qualquer outro elemento do ambiente que não esteja se choccando com o sensor.

Quanto as ações que o agente pode fazer são apenas 3: acelerar, frear e direcionar as rodas a direita e a esquerda. A primeira e a última ação são chamadas ações contínuas, sendo assim um número que indica o nível de aceleração que o carro irá produzir, e no caso da direção da roda o quão direciona elas estão a esquerda ou direita. O ato de frear por outro lado é uma grandeza discreta, o veículo está ou não está freando.

\subsection{As recompensas}
Para compreender as recompensas é necessário distinguir as recompensas que são aplicadas durante o episódio das que são aplicadas ao fim do mesmo. Ambas são importantes para a otimização do comportamento do agente. A política do agente é atualizada dentro do episódio e também um após o outro, por isso receber a recompensa ou punição durante o episódio é necessário para que o agente saiba quais estados devem ser almejados e quais devem ser evitados.

Os eventos sujeitos a aplicação de recompensa ou punição que não definem o fim do episódio e sua representação matemática são os seguintes:

\begin{itemize}
   \item Passar por um \textit{checkpoint} ($r_c$);
   \item Colidir com a parede ($p_p$);
   \item Colidir com a calçada ($p_c$);
   \item \textit{Step} ($p_{step}$)
\end{itemize}

Os \textit{steps} são as etapas explicadas no capítulo anterior, cada um possui um estado em que o agente se encontra e permite que ele tome uma decisão. O $p_{step}$ é importante para que o agente não fique parado, como se estivesse ``receoso'' de receber uma punição, aplicando uma penalidade pequena por \textit{step} faz com que ele procure por recompensas. 

Quanto aos eventos que definem o fim do episódio são três:
\begin{itemize}
   \item Chegar ao destino ($R_d$);
   \item Tempo se esgotar ($P_{step}$);
   \item Tombar o veículo ($P_t$);
\end{itemize}

Se ao fim de um episódio, o agente chegar ao destino antes do tempo se esgotar, ele deve receber a recompensa máxima. Caso o tempo se esgote o agente deve receber uma punição. Em ambos os casos será considerado o número de \textit{checkpoints} atingidos (o agente pode se direcionar até o destino ignorando-os), bem como as colisões com a parede e calçada. Desta forma, podemos avaliar a qualidade com que o agente conduziu o veículo. Por exemplo, se ele chegou ao destino se chocando com a calçada ou se fez um trajeto ``limpo''; outro exemplo, no segundo cenário, o agente receberá uma recompensa maior caso chegue mais perto do destino do que se atingir nenhum \textit{checkpoint}. Por fim, se o agente tombar o veículo ele receberá a punição máxima, independente de seu comportamento antes do tal evento.

Tendo isso em vista, para formalização matemática dos episódios temos o seguinte:

\begin{itemize}
   \item Chega ao destino: $R_T = R_d + R_c - P_c - P_p$
   \item Não chega a tempo: $R_T = R_c - P_{step} - P_c - P_p$
   \item Tomba o carro: $R_T = P_t$
\end{itemize}

Onde $R_t$ é a recompensa final do episódio. $r_c \doteq \frac{R_c}{n_c}$, com $n_c$ sendo o número de \textit{checkpoints} no trajeto específico. Similarmente, temos $P_c \doteq p_c * n$ e $P_p \doteq p_p * n$, que são as punições totais (o produto da punição do evento pelo número de ocorrências $n$) pelas colisões com a calçada e a parede, respectivamente. Além disso, o agente possui 800 \textit{steps} para atingir o destino. Este limite é aproximadamente 15 segundos, tempo suficiente dado que todos os trajetos são curtos. Com isso $P_{step} \doteq p_{step} * 800$.

As recompensas variam de -1 a 1, este intervalo não deve ser ultrapassado pois pode levar a instabilidade no treino. Como foi visto, algumas das recompensas/punições acima depende de outras. A tabela abaixo exibe o valor de todas as constantes de recompensas, estas que foram definidas e ajustadas pelo autor até gerar um treino mais estável. 

\begin{table}[htpb]
   \centering
   \caption{Valor das recompensas e punições dos eventos.}
   \label{tabela-recompensas}
   \begin{tabular}{|l|p{4cm}|r|}
        \hline
        \small{Evento}                           & \small{Constante}                              &   \small{Valor}     \\ \hline
         Atingir todos \textit{checkpoints}      & $R_c$                                          &    0,8              \\ \hline
         Chegar ao destino                       & $R_d$                                          &    0,2              \\ \hline
         Colidir com a calçada                   & $p_c$                                          &    -0,05            \\ \hline
         Colidir com a parede                    & $p_p$                                          &    -0,05            \\ \hline
         Esgotar o tempo                         & $P_{step}$                                     &    -0,7             \\ \hline
         Tombar o veículo                        & $P_t$                                          &    -1               \\ \hline
   \end{tabular}
\end{table}

% .

% Os eventos sujeitos a aplicação de recompensa/punição são os seguintes: o veículo chegar ao destino ($R_d$), o veículo atingir um checkpoint ($r_c$), colidir com a calçada ($p_c$), colidir com a parede($p_p$), o agente tombar o veículo ($P_t$) e por fim a punição total por \textit{step} $P_{step}$. $R_d$, e $P_t$ são constantes e são eventos que definem o fim do episódio. Por outro lado, $r_c, p_c, p_p \text{ e } p_{step}$ serão aplicadas ao longo do episódio conforme os eventos ocorrerem, no caso do último o evento é o próprio \textit{step} e seu valor é $p_{step} = \frac{P_{step}}{N}$ sendo $N$ o número máximo de \textit{steps} por episódio.

% Para o encerramento do episódio temos os seguintes cenários possíveis: o agente chega ao destino, o agente não chega a tempo, o agente tomba o veículo. Para o primeiro cenário, caso o agente passe por todos os \textit{checkpoints} e não colida com nenhum obstáculo ele deve receber a recompensa máxima. Porém, para cada colisão sua recompensa será descontada, assim como se não passar por todos os \textit{checkpoints}. A punição pelo tempo não está presente aqui pois ela não é necessária caso neste cenário, se o agente chegou ao destino antes do tempo limite, então ele não deve ser punido por isso. O $p_{step}$ só é aplicado dentro do episódio, a recompensa final do episódio no primeiro cenário é sobrescrita sem levar

% Caso o agente não chegue a tempo, ele receberá a punição $P_{step}$ mais as punições por colisões, mas os \textit{checkpoints} pelo qual passou devem ser considerados. Isso serve para premiar episódios 
% As recompensas estão no intervalo $[-1,1]$ e serão aplicadas da seguinte forma em cada cenário:



% Sendo $R_T$ a recompensa total do episódio, $R_c$, $P_c$ e $P_p$ representam a soma total de ocorrências de $r_c$, $p_c$ e $p_p$, respectivamente. Vale ressaltar que $R_d + R_c = 1$, isto quer dizer que se o veículo chegar ao destino sem se chocar com a calçada ou parede, terá a recompensa máxima, por outro lado $P_t = -1$, indicando que caso o veículo tombe, não interessa quantos checkpoints ele atingiu, a punição é máxima. Podemos interpretar o segundo cenário como: caso o veículo não chegue a tempo recebe uma recompensa pelo progresso ($R_c$) descontado com punição por tempo mais as punições por colisões. Sobre $r_c$, seu valor está sujeito ao trajeto do episódio, como eles possuem tamanho diferente, o valor a recompensa segue na forma $r_{c_x} \doteq \frac{R_c}{n_{c_x}}$, sendo $n_{c_x}$ a quantidade de checkpoints naquela rota $x$.

\subsection{O veículo}
Já vimos como funciona o agente, agora vamos destrinchar como funciona a condução do \textit{GameObject} do veículo. O que foi explicado em \nameref{agent-subsection} se trata dos componentes que estão associados ao objeto do topo da hierarquia, o \textit{GameObject Car}. Abaixo do mesmo há dois objetos \textit{body} e \textit{spoiler} que são basicamente os \textit{meshes 3D} que dão o visual do veículo.

\begin{figure}[h]
   \centering
   \includegraphics{figs/hierarquia-veiculo.png}
    \caption{Hierarquia dos \textit{GameObjects} que compõe o veículo.}
    \label{fig:vehicle-hierarchy}
\end{figure}

O terceiro objeto é \textit{Wheels} que agrega os objetos \textit{Meshes} e \textit{Wheel Colliders}, o primeiro é um conjunto dos \textit{meshes 3D} das rodas, o segundo agrupa os \textit{colliders} das mesmas. Estes últimos possuem um \textit{component} chamado \textit{Wheel Collider}, que é responsável pela física da roda (\citeonline{wheelcollider}) podendo configurar o raio, massa, amortecimento, etc. Todos os atributos estão com o valor padrão, exceto o raio que foi alinhado com o do \textit{mesh 3D}.

Para o controle do veículo, foi adaptado um código fonte disponível em (\citeonline{PrismYoutube_2022}), este código pega o input do usuário e atribui os mesmos a propriedades padrões dos \textit{colliders} das rodas dianteiras, o input ``vertical'' é aplicado ao torque do motor (\textit{motorTorque}), o ``horizonta'' ao ângulo de esterçamento do veículo (\textit{steeringAngle}, limitado a 30°). Ao frear é aplicado uma \textit{breakforce} nas quatro rodas. \textit{breakforce}, \textit{motorTorque} e \textit{steeringAngle} são todos propriedades do componente \textit{Wheel Collider} da Unity, basta associar os inputs do usuário a eles que a \textit{engine} da Unity3D lida com a física do veículo.

Para o projeto, foi feita uma adaptação neste código-fonte, em vez de se aplicar o input do usuário, foi aplicado os valores que vem do \textit{actionBuffer}, que é um argumento de \textit{OnActionReceived()} a função que é responsável por tratar os dados das ações tomadas pelo agente. O \textit{actionBuffer} é um array que contém em cada elemento os valores da política naquele estado, isto é, o ``input do agente''.

O agente no projeto pode ser entendido como o conjunto de \textit{components} que fazem parte do GameObject pai, o ``Car'', seriam eles: behavior Parameters, o Car Agent, o Decision Requester e o Ray Perception sensor 3D. 

\section{Os algoritmos e hiperparâmetros}\label{algoritmos}
Para executar o treino é necessário especificar para o \textbf{ml-agents} o algoritmo e suas configurações, para isso existe um arquivo \textbf{.yaml} que é responsável por isso. Abaixo, um exemplo seguido da explicação de cada parâmetro:

\begin{lstlisting}
behaviors:
   MoveToTarget:
      trainer_type: ppo
      max_steps: 9600000
      time_horizon: 64
      summary_freq: 60000
      keep_checkpoints: 16     
      checkpoint_interval: 600000
      hyperparameters:
         learning_rate: 1.0e-5
         batch_size: 1024
         buffer_size: 10240
         beta: 5.0e-2
         epsilon: 0.1
         lambd: 0.99
         num_epoch: 8
         learning_rate_schedule: linear
         beta_schedule: linear
         epsilon_schedule: linear
      network_settings:
         normalize: false
         hidden_units: 64
         num_layers: 2
      reward_signals:
         extrinsic:
            gamma: 0.99
            strength: 1.0
\end{lstlisting}

O arquivo começa com o \textbf{behaviors} que é uma lista de configurações dos comportamentos do agente, neste projeto haverá apenas um que é chamado \textbf{MoveToTarget}. Abaixo segue uma lista dos hiperparâmetros mais relevantes, o texto aqui é em grande parte traduzido da documentação oficial do \textbf{ml-agents} (\citeonline{juliani2020}) com algumas remoções de informações não relevantes pra este projeto e com alguns adendos do autor caso necessário.

\begin{description}
   \item [trainer\_type:] O algoritmo que será usado, neste projeto serão PPO ou SAC
   \item [max\_steps:] O número máximo de passos (observações coletadas e ações tomadas pelo agente) tomados  no ambiente. No simulador a tarefa sempre será episódica, o tamanho do episódio pode variar nos ajustes, mas cada um possui em torno de 1200 \textit{steps}.
   \item [time\_horizon:] O número de \textit{steps} anteriores a ser coletado por agente para adicionar ao \textit{buffer} de experiência. Quando este limite é alcançado antes do fim de um episódio, um valor estimado é usado para prever a expectativa geral de recompensa a partir do estado atual do agente. Por isso, esse parâmetro varia entre menos enviesado mas com alta variância estimada (longo prazo) e mais enviesado mas com estimativa de menor variância (curto prazo). Em casos onde há frequente sinais de recompensas ou em episódios muito longos, um número menor pode ser mais ideal.
   \item [hyperparameters->learning\_rate:] Taxa inicial do salto a cada atualização do gradiente descendente. Este número geralmente deve diminuir com se o treino está instável e a recompensa com aumenta consistentemente.
   \item [hyperparameters->batch\_size:] O número de experiências coletadas para cada atualização do gradiente descendente. Em caso de usar ações contínuas ele deve estar na casa do milhares, caso contrário na casa das dezenas deve bastar.
   \item [hyperparameters->batch\_size:] Número de experiências a ser coletada antes de atualizar o modelo da política. Tipicamente, um buffer size maior corresponde a um treino mais estável. No caso do SAC este número deve ser milhares de vezes maior que um episódio típico, pois o algoritmo deve aprender de experiências velhas quanto mais novas.
   \item [hyperparameters->beta:] (somente PPO) força da regularização da entropia, que faz com que a política seja ``mais aleatória''. Isto garente que o agente explore apropriadamente o espaço da ação durante o treino. Aumentá-lo faz com que o agente tome mais ações aleatória. Isto deve ser ajustado de modo que o a entropia (medido pelo tensorboard) lentamente decresça conforme aumenta a recompensa. Se a entropia cair muito rápido aumente o \textbf{beta}, caso demore demais diminua-o.
   \item [hyperparameters->epsilon:] (somente PPO) Influencia no quão rapidamente a política pode evoluir durante o treino. Corresponde ao limite da divergência entre as velhas e novas políticas durante a atualização do gradiente descendente. Um valor menor leva a atualizações mais estáveis, mas deixa o processo de aprendizagem mais lento.
   \item [hyperparameters->lambd:] (somente PPO) parâmetro de regularização (lambda) usado quando calculado o estimador de valor generalizado (GAE (\citeonline{1506.02438})). Isto pode ser entendido em o quanto o agente depende no seu atual valor estimado quando atualizando o valor estimado. Valores baixos correspondem à apoiar-se mais no valor atual (alto viés/\textit{bias}) e valores elevados corresponde a confiar mais nas recompensas recebidas pelo ambiente (que pode causar alta variância). (geralmente varia de 0,9 a 0,99)
   \item [hyperparameters->num\_epoch:] (somente PPO) O número de passagens a fazer pelo buffer\_size quando otimizando gradiente descendente. Este deve crescer semelhante ao batch\_size. Diminui-lo tende a ter atualizações mais estáveis, aumentá-lo tem-se um treino mais lento. (geralmente varia entre 3 a 10)
   \item [hyperparameters->learning\_rate\_schedule:] Determina se o valor do learning\_rate muda durante o treino, os valores podem ser \textit{linear} ou \textit{constant}. Sendo o primeiro ele irá decair linearmente até zero quanto executar o \textit{max\_steps}. O segundo mantém o valor constante. É recomendado adotar \textit{linear} para PPO e o outro quando usar SAC.
   \item [hyperparameters->beta\_schedule:] (somente PPO) Similarmente ao acima mas para o \textbf{beta}. 
   \item [hyperparameters->epsilon\_schedule:] (somente PPO) Similarmente ao acima mas para o \textbf{epsilon}. 
   \item [hyperparameters->buffer\_init\_steps:] (somente SAC) Número de experiências a coletar no buffer antes de atualizar o modelo da política. Inicialmente o buffer é preenchido com ações aleatórias, o que é muito útil para exploração. Este número deve estar na casa de dezenas de vezes maior que um típico episódio.
   \item [hyperparameters->init\_entcoef:] (somente SAC) Corresponde a entropia inicial definida no começo do treino e é o quanto o agente deve explorar inicialmente. No SAC, o agente é incentivado a tomar decisões aleatorias a fim de explorar melhor o ambiente. O coeficiente de entropia é ajustado automaticamente para um valor alvo predefinido então o \textit{init\_entcoef} é apenas o valor inicial. Quanto maior o valor maior a exploração inicial. Para ações contínuas o típico valor está no invervalo 0,5-1,0, discreto em 0,05-0,5.
   \item [network\_settings->hidden\_units:] O número de nós em cada camada intermediária da rede neural totalmente conectada (valores típicos giram em torno de 32 a 512).
   \item [network\_settings->num\_layers:] O número de camadas intermediárias na rede neural (tipicamente tem valor de 1 a 3).
   \item [network\_settings->normalize:] Se normalização deve ser aplicada no vetor de observação. Esta normalização pode melhorar o treino em caso de tarefas que lidam com controles contínuos complexos, mas pode atrapalhar caso contrário.
\end{description}

\section{Desenvolvimento}\label{metodologia}
O objetivo final é chegar a um agente que saiba percorrer todos os 17 trajetos de forma eficiente, para isso é preciso analisar as estatísticas produzidas pelo treino, saber se a política se aproximou suficientemente do comportamento ótimo de modo que não cometa erros. Antes que se chegue a este nível é preciso que ele demonstre sucesso em treinos mais simples, então dividimos as etapas em 3 \textbf{desafios}. Os dois primeiros serão em trajetos específicos, mas com níveis diferentes de dificuldade. O último será o treino o desafio geral, onde ele deve percorrer qualquer rota dada a ele.

% Como foi dito na seção \nameref{sec:objetivos}, o estudo não propõe um modelo imutável, ao invés disso, é esperado que o agente evolua em diversas frentes: seus sensores, os métodos de treino, os algoritmos, o ambiente envolvido se torne mais realista, entre outros. Como será visto no Capítulo \ref{cap:resultados}, o agente e ambiente passou por mudanças a fim de cumprir as tarefas, estas alterações serão justificadas e compreendidas com os resultados dos treinos e testes.

\subsection{Treinamento e teste}
O treinamento é executado via linha de comando, após sua conclusão é gerado um arquivo \textbf{.onnx} que é política final do agente, isto é, a rede neural com os coeficientes gerados pelo algoritmo durante o treino, o seu ``cérebro''. Finalmente podemos por o agente em teste, de volta ao editor da Unity3D, basta carregar o cérebro no agente e executar o teste, o veículo agora se move de acordo como a política traduz os estados em ações, definida no arquivo .onnx, sem qualquer input do usuário, com o agente agindo de acordo com os resultados do treino, seja ele um sucesso ou não. 

Quando o treino é finalizado, suas estatísticas são exibidas pelo \textbf{tensorboard}, um pacote Python incluso no pacote ml-agents. Nelas é possível ter uma visualização do desempenho do agente durante o treino e sua curva de aprendizado. Quais são e o que significam estão na documentação do ML-agents em (\citeonline{juliani2020}). A estatística que iremos analisar aqui será o gráfico de \textbf{recompensa acumulada} (cumulative reward) que exibe a recompensa mediana por episódio, isso nos fará entender se o agente convergiu e com que velocidade. As demais estatísticas não tem muita utilidade para o nosso propósito, elas são mais úteis para saber como ajustar os hiperparâmetros em caso de um treino não sucedido.

Nos testes o agente deverá percorrer os mesmos trajetos nos quais treinou, como as observações do agente não incluem qualquer informação sobre qual trajeto ele está percorrendo no momento, o autor não viu necessidade de separar os trajetos em um conjuntos de treino e teste, porém isso pode ser explorado em estudos futuros. 

O primeiro desafio inclui dois treinos um em cada trajeto fácil diferente, por fácil entende-se uma rota que seja necessário fazer apenas uma curva. O segundo desafio inclui três treinos em rotas com duas ou mais curvas. Por fim o desafio geral, onde ele treinará em todos os dezessete trajetos. Após obtermos o ``cérebro'' gerado, este será colocado em teste percorrendo o mesmo trajeto o qual treinou (no caso dos desafios específicos), percorrendo-os dez vezes. No caso do desafio geral, 3 vezes em cada rota.

O consolidado dos desafios está resumido na Tabela \ref{tabela-testes}, a segunda coluna ``Rotas'' se refere ao nome dado ao \textit{GameObject} da rota dentro do editor Unity.

\begin{table}[htpb]
   \centering
   \caption{Resumo dos desafios}
   \label{tabela-testes}
   \begin{tabular}{|l|p{4cm}|r|}
        \hline
        \small{Desafio}                      & \small{Rotas}                                                         & \small{Qtde. testes}     \\ \hline
         Específico em trajetos fáceis       & \textit{Path(8)}\newline \textit{Path(0)}                             &      10 em cada rota     \\ \hline
         Específico em trajetos difíceis     & \textit{Path(3)}\newline \textit{Path(5)}\newline \textit{Path(6)}    &      10 em cada rota     \\ \hline
         Geral                               & Todas as 17 rotas                                                     &      51 (3 em cada)      \\ \hline
   \end{tabular}
\end{table}

\subsection{Análise}
Para cada treino será testado três algoritmos: PPO, BC e GAIL. Então o primeiro é um treino/teste com apenas RL, enquanto os outros dois se tratam de Aprendizado por Imitação. A análise de desempenho avaliará os seguintes quesitos:

\begin{itemize}
   \item Se convergiu ou não;
   \item Mediana final de recompensa por episódio;
   \item Porcentagem de conclusão da rota nos testes (cada rota no geral);
   \item Média de recompensa na rota (caso treino em rota especifíca);
   \item Média de recompensa por rota (caso geral).
\end{itemize}


Ao fim devemos ser capazes de responder as seguintes perguntas:
\begin{itemize}
   \item Foi possível ensinar um agente a percorrer trajetos?
   \item Qual algoritmo mais eficiente nos testes? 
   \item Qual algoritmo convergiu mais rápido nos treinos?
\end{itemize}